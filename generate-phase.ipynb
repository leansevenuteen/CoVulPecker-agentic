{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T07:57:10.578746Z",
     "iopub.status.busy": "2025-11-28T07:57:10.578334Z",
     "iopub.status.idle": "2025-11-28T07:57:10.584803Z",
     "shell.execute_reply": "2025-11-28T07:57:10.584036Z",
     "shell.execute_reply.started": "2025-11-28T07:57:10.578727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"VLLM_USE_FLASHINFER\"] = \"0\"\n",
    "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"TRITON_ATTN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T07:57:10.586472Z",
     "iopub.status.busy": "2025-11-28T07:57:10.586271Z",
     "iopub.status.idle": "2025-11-28T07:57:10.831950Z",
     "shell.execute_reply": "2025-11-28T07:57:10.831051Z",
     "shell.execute_reply.started": "2025-11-28T07:57:10.586457Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLLM_USE_FLASHINFER=0\n"
     ]
    }
   ],
   "source": [
    "!env | grep PYTORCH_CUDA_ALLOC_CONF\n",
    "!env | grep VLLM_USE_FLASHINFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T07:57:10.833249Z",
     "iopub.status.busy": "2025-11-28T07:57:10.832891Z",
     "iopub.status.idle": "2025-11-28T07:57:31.703466Z",
     "shell.execute_reply": "2025-11-28T07:57:31.702787Z",
     "shell.execute_reply.started": "2025-11-28T07:57:10.833217Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Collecting scikit-learn==1.5.2\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2.4.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy==1.26.4) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy==1.26.4) (2024.2.0)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m238.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.5.2\n",
      "Collecting transformers==4.56.2\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers==4.56.2)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.2) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.2) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.2) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.2) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.2) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.2) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.2) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.2) (2025.10.5)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.56.2) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.56.2) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.56.2) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.56.2) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.56.2) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.56.2) (2024.2.0)\n",
      "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.3\n",
      "    Uninstalling transformers-4.53.3:\n",
      "      Successfully uninstalled transformers-4.53.3\n",
      "Successfully installed tokenizers-0.22.1 transformers-4.56.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U numpy==1.26.4 scikit-learn==1.5.2 --no-cache-dir\n",
    "!pip install transformers==4.56.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T07:57:31.704551Z",
     "iopub.status.busy": "2025-11-28T07:57:31.704313Z",
     "iopub.status.idle": "2025-11-28T08:01:40.334418Z",
     "shell.execute_reply": "2025-11-28T08:01:40.333446Z",
     "shell.execute_reply.started": "2025-11-28T07:57:31.704526Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\n",
      "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "ydata-profiling 4.17.0 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\n",
      "ydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting flask_cors\n",
      "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.3.1)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.0.3)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.3)\n",
      "Downloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
      "Downloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: pyngrok, flask_cors\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [flask_cors]\n",
      "\u001b[1A\u001b[2KSuccessfully installed flask_cors-6.0.1 pyngrok-7.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade pip\n",
    "!pip install -q vllm==0.11.1 \n",
    "!pip install -q huggingface_hub\n",
    "!pip install flask pyngrok flask_cors\n",
    "!pip install -q bitsandbytes accelerate\n",
    "# print(\"--- [FIX] B·∫Øt ƒë·∫ßu s·ª≠a l·ªói xung ƒë·ªôt numpy/sklearn ---\")\n",
    "# # # Bu·ªôc c√†i ƒë·∫∑t l·∫°i c√°c g√≥i b·ªã ·∫£nh h∆∞·ªüng ƒë·ªÉ ƒë·∫£m b·∫£o ch√∫ng t∆∞∆°ng th√≠ch\n",
    "# !pip install --upgrade --force-reinstall scikit-learn\n",
    "# \n",
    "# print(\"--- [FIX] ƒê√£ ho√†n t·∫•t s·ª≠a l·ªói ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:01:40.335881Z",
     "iopub.status.busy": "2025-11-28T08:01:40.335584Z",
     "iopub.status.idle": "2025-11-28T08:01:40.452279Z",
     "shell.execute_reply": "2025-11-28T08:01:40.451334Z",
     "shell.execute_reply.started": "2025-11-28T08:01:40.335852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:01:40.453630Z",
     "iopub.status.busy": "2025-11-28T08:01:40.453366Z",
     "iopub.status.idle": "2025-11-28T08:01:42.169617Z",
     "shell.execute_reply": "2025-11-28T08:01:42.169002Z",
     "shell.execute_reply.started": "2025-11-28T08:01:40.453607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from pyngrok import ngrok\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import snapshot_download\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "from openai import OpenAI\n",
    "from typing import List, Set, Dict, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:01:42.172997Z",
     "iopub.status.busy": "2025-11-28T08:01:42.172231Z",
     "iopub.status.idle": "2025-11-28T08:01:42.236466Z",
     "shell.execute_reply": "2025-11-28T08:01:42.235753Z",
     "shell.execute_reply.started": "2025-11-28T08:01:42.172971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"hf_token\")\n",
    "if hf_token is None:\n",
    "    raise RuntimeError(\"Thi·∫øu HF_TOKEN. Vui l√≤ng th√™m v√†o Kaggle Secrets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:01:42.237801Z",
     "iopub.status.busy": "2025-11-28T08:01:42.237228Z",
     "iopub.status.idle": "2025-11-28T08:02:49.920585Z",
     "shell.execute_reply": "2025-11-28T08:02:49.919685Z",
     "shell.execute_reply.started": "2025-11-28T08:01:42.237774Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒêƒÉng nh·∫≠p Hugging Face th√†nh c√¥ng!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518ce7d8e1ab49d3910ecd3ebcc93d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c762678ec65462b8b4e5b267d8229a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edccd31b7eda476598a5b64e58bc9c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a5ded6b3144185be5cb8f202f70dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b6c0aeb3a34f9a90f547afad7137bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881cfa33d31542739aadd36f860a8994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d7e4c5e30841e2879cc2a520554154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818e687cc7ae4a8b840b9da72c545875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed85a9104144755a5589d911c379746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7dea09368274bf3b595b5332c1b8dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa00bddaf07643e7818a6348812995b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef90396d965c40308a89f7cc38c6088d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/3.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e9c65ee797470eaa1febf9b227d8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d8dfb52fe04aeb818d50d39a20273c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f550cf4ec64b708468654232d30201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060ae4be5d004c13916a5114da259a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ download xong v√†o: /kaggle/tmp/Qwen2.5-32B-Instruct-AWQ\n"
     ]
    }
   ],
   "source": [
    "login(token=hf_token)\n",
    "print(\"‚úÖ ƒêƒÉng nh·∫≠p Hugging Face th√†nh c√¥ng!\")\n",
    "# √î¬†2b: T·∫£i model safetensors t·ª´ repo gated\n",
    "\n",
    "\n",
    "# MODEL_REPO = \"ktam204/Qwen3-32B-AWQ-r16-lora-all-Pentest-swift\"\n",
    "# MODEL_REPO = \"Qwen/Qwen3-32B-AWQ\"\n",
    "# LOCAL_DIR   = \"/kaggle/tmp/Qwen3-32B\"\n",
    "\n",
    "# MODEL_REPO = \"cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit\"\n",
    "MODEL_REPO = \"Qwen/Qwen2.5-32B-Instruct-AWQ\"\n",
    "# MODEL_REPO = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "LOCAL_DIR = \"/kaggle/tmp/Qwen2.5-32B-Instruct-AWQ\"\n",
    "\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=MODEL_REPO,\n",
    "    local_dir=LOCAL_DIR,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "print(\"‚úÖ ƒê√£ download xong v√†o:\", LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:02:49.921675Z",
     "iopub.status.busy": "2025-11-28T08:02:49.921447Z",
     "iopub.status.idle": "2025-11-28T08:02:50.529522Z",
     "shell.execute_reply": "2025-11-28T08:02:50.528657Z",
     "shell.execute_reply.started": "2025-11-28T08:02:49.921658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 450 (4819.4 MB)\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:02:50.531103Z",
     "iopub.status.busy": "2025-11-28T08:02:50.530815Z",
     "iopub.status.idle": "2025-11-28T08:02:51.403225Z",
     "shell.execute_reply": "2025-11-28T08:02:51.402365Z",
     "shell.execute_reply.started": "2025-11-28T08:02:50.531078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !ls $LOCAL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:02:51.404501Z",
     "iopub.status.busy": "2025-11-28T08:02:51.404278Z",
     "iopub.status.idle": "2025-11-28T08:02:52.442849Z",
     "shell.execute_reply": "2025-11-28T08:02:52.441951Z",
     "shell.execute_reply.started": "2025-11-28T08:02:51.404484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !cat /usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:02:52.444499Z",
     "iopub.status.busy": "2025-11-28T08:02:52.443902Z",
     "iopub.status.idle": "2025-11-28T08:05:53.428401Z",
     "shell.execute_reply": "2025-11-28T08:05:53.427692Z",
     "shell.execute_reply.started": "2025-11-28T08:02:52.444467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on attempt 1: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a2404cd190>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "\n",
      "Waiting for vLLM startup... (1/360)\n",
      "Error on attempt 2: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a2404dd450>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "\n",
      "Waiting for vLLM startup... (2/360)\n",
      "Error on attempt 3: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a25b7eb310>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "2025-11-28 08:03:10.113769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764316990.290399     169 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764316990.343288     169 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\n",
      "Waiting for vLLM startup... (3/360)\n",
      "Error on attempt 4: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a2404e2490>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/api_server.py\", line 40, in <module>\n",
      "    from vllm.config import VllmConfig\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config/__init__.py\", line 17, in <module>\n",
      "    from vllm.config.model import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config/model.py\", line 24, in <module>\n",
      "    from vllm.transformers_utils.config import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/transformers_utils/config.py\", line 28, in <module>\n",
      "    from transformers.models.auto.image_processing_auto import get_image_processor_config\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/image_processing_auto.py\", line 27, in <module>\n",
      "    from ...image_processing_utils import ImageProcessingMixin\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\", line 22, in <module>\n",
      "    from .image_transforms import center_crop, normalize, rescale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\", line 48, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\", line 467, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/__init__.py\", line 2, in <module>\n",
      "    from keras.api import DTypePolicy\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/__init__.py\", line 34, in <module>\n",
      "    from keras.api import visualization\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n",
      "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
      "    from matplotlib import patches  # For legend patches\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "\n",
      "Waiting for vLLM startup... (4/360)\n",
      "Error on attempt 5: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a2404ec210>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/api_server.py\", line 40, in <module>\n",
      "    from vllm.config import VllmConfig\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config/__init__.py\", line 17, in <module>\n",
      "    from vllm.config.model import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config/model.py\", line 24, in <module>\n",
      "    from vllm.transformers_utils.config import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/transformers_utils/config.py\", line 28, in <module>\n",
      "    from transformers.models.auto.image_processing_auto import get_image_processor_config\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/image_processing_auto.py\", line 27, in <module>\n",
      "    from ...image_processing_utils import ImageProcessingMixin\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\", line 22, in <module>\n",
      "    from .image_transforms import center_crop, normalize, rescale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\", line 48, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\", line 467, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/__init__.py\", line 2, in <module>\n",
      "    from keras.api import DTypePolicy\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/__init__.py\", line 34, in <module>\n",
      "    from keras.api import visualization\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n",
      "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
      "    from matplotlib import patches  # For legend patches\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "INFO 11-28 08:03:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "\u001b[1;36m(APIServer pid=169)\u001b[0;0m INFO 11-28 08:03:27 [api_server.py:1977] vLLM API server version 0.11.1\n",
      "\u001b[1;36m(APIServer pid=169)\u001b[0;0m INFO 11-28 08:03:27 [utils.py:253] non-default args: {'host': '0.0.0.0', 'model': '/kaggle/tmp/Qwen2.5-32B-Instruct-AWQ', 'dtype': 'float16', 'max_model_len': 16384, 'enforce_eager': True, 'served_model_name': ['Qwen2.5-32B-Instruct-AWQ'], 'tensor_parallel_size': 2}\n",
      "\n",
      "Waiting for vLLM startup... (5/360)\n",
      "Error on attempt 6: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a2404e3ad0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/api_server.py\", line 40, in <module>\n",
      "    from vllm.config import VllmConfig\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config/__init__.py\", line 17, in <module>\n",
      "    from vllm.config.model import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config/model.py\", line 24, in <module>\n",
      "    from vllm.transformers_utils.config import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/transformers_utils/config.py\", line 28, in <module>\n",
      "    from transformers.models.auto.image_processing_auto import get_image_processor_config\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/image_processing_auto.py\", line 27, in <module>\n",
      "    from ...image_processing_utils import ImageProcessingMixin\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\", line 22, in <module>\n",
      "    from .image_transforms import center_crop, normalize, rescale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\", line 48, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\", line 467, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/__init__.py\", line 2, in <module>\n",
      "    from keras.api import DTypePolicy\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/__init__.py\", line 34, in <module>\n",
      "    from keras.api import visualization\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n",
      "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
      "    from matplotlib import patches  # For legend patches\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "INFO 11-28 08:03:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "\u001b[1;36m(APIServer pid=169)\u001b[0;0m INFO 11-28 08:03:27 [api_server.py:1977] vLLM API server version 0.11.1\n",
      "\u001b[1;36m(APIServer pid=169)\u001b[0;0m INFO 11-28 08:03:27 [utils.py:253] non-default args: {'host': '0.0.0.0', 'model': '/kaggle/tmp/Qwen2.5-32B-Instruct-AWQ', 'dtype': 'float16', 'max_model_len': 16384, 'enforce_eager': True, 'served_model_name': ['Qwen2.5-32B-Instruct-AWQ'], 'tensor_parallel_size': 2}\n",
      "\n",
      "Waiting for vLLM startup... (6/360)\n",
      "Error on attempt 7: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a2404e27d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/api_server.py\", line 40, in <module>\n",
      "    from vllm.config import VllmConfig\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config/__init__.py\", line 17, in <module>\n",
      "    from vllm.config.model import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config/model.py\", line 24, in <module>\n",
      "    from vllm.transformers_utils.config import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/transformers_utils/config.py\", line 28, in <module>\n",
      "    from transformers.models.auto.image_processing_auto import get_image_processor_config\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/image_processing_auto.py\", line 27, in <module>\n",
      "    from ...image_processing_utils import ImageProcessingMixin\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\", line 22, in <module>\n",
      "    from .image_transforms import center_crop, normalize, rescale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\", line 48, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\", line 467, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/__init__.py\", line 2, in <module>\n",
      "    from keras.api import DTypePolicy\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/__init__.py\", line 34, in <module>\n",
      "    from keras.api import visualization\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n",
      "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
      "    from matplotlib import patches  # For legend patches\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "INFO 11-28 08:03:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "\u001b[1;36m(APIServer pid=169)\u001b[0;0m INFO 11-28 08:03:27 [api_server.py:1977] vLLM API server version 0.11.1\n",
      "\u001b[1;36m(APIServer pid=169)\u001b[0;0m INFO 11-28 08:03:27 [utils.py:253] non-default args: {'host': '0.0.0.0', 'model': '/kaggle/tmp/Qwen2.5-32B-Instruct-AWQ', 'dtype': 'float16', 'max_model_len': 16384, 'enforce_eager': True, 'served_model_name': ['Qwen2.5-32B-Instruct-AWQ'], 'tensor_parallel_size': 2}\n",
      "\u001b[1;36m(APIServer pid=169)\u001b[0;0m INFO 11-28 08:03:45 [model.py:631] Resolved architecture: Qwen2ForCausalLM\n",
      "\u001b[1;36m(APIServer pid=169)\u001b[0;0m INFO 11-28 08:03:45 [model.py:1745] Using max model len 16384\n",
      "\u001b[1;36m(APIServer pid=169)\u001b[0;0m INFO 11-28 08:03:47 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "\u001b[1;36m(APIServer pid=169)\u001b[0;0m INFO 11-28 08:03:47 [vllm.py:500] Cudagraph is disabled under eager mode\n",
      "\n",
      "Waiting for vLLM startup... (7/360)\n",
      "Error on attempt 8: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a25b7eab10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 131, in _main\n",
      "    prepare(preparation_data)\n",
      "  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 244, in prepare\n",
      "    _fixup_main_from_name(data['init_main_from_name'])\n",
      "  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 268, in _fixup_main_from_name\n",
      "    main_content = runpy.run_module(mod_name,\n",
      "  File \"<frozen runpy>\", line 226, in run_module\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/api_server.py\", line 40, in <module>\n",
      "    from vllm.config import VllmConfig\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config/__init__.py\", line 17, in <module>\n",
      "    from vllm.config.model import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config/model.py\", line 24, in <module>\n",
      "    from vllm.transformers_utils.config import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/transformers_utils/config.py\", line 28, in <module>\n",
      "    from transformers.models.auto.image_processing_auto import get_image_processor_config\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/image_processing_auto.py\", line 27, in <module>\n",
      "    from ...image_processing_utils import ImageProcessingMixin\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\", line 22, in <module>\n",
      "    from .image_transforms import center_crop, normalize, rescale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\", line 48, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\", line 467, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/__init__.py\", line 2, in <module>\n",
      "    from keras.api import DTypePolicy\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/__init__.py\", line 34, in <module>\n",
      "    from keras.api import visualization\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n",
      "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
      "    from matplotlib import patches  # For legend patches\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "\u001b[1;36m(EngineCore_DP0 pid=206)\u001b[0;0m INFO 11-28 08:04:01 [core.py:93] Initializing a V1 LLM engine (v0.11.1) with config: model='/kaggle/tmp/Qwen2.5-32B-Instruct-AWQ', speculative_config=None, tokenizer='/kaggle/tmp/Qwen2.5-32B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen2.5-32B-Instruct-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=206)\u001b[0;0m WARNING 11-28 08:04:01 [multiproc_executor.py:869] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\n",
      "Waiting for vLLM startup... (8/360)\n",
      "Error on attempt 9: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a24049f590>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 131, in _main\n",
      "    prepare(preparation_data)\n",
      "  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 244, in prepare\n",
      "    _fixup_main_from_name(data['init_main_from_name'])\n",
      "  File \"/usr/lib/python3.11/multiprocessing/spawn.py\", line 268, in _fixup_main_from_name\n",
      "    main_content = runpy.run_module(mod_name,\n",
      "  File \"<frozen runpy>\", line 226, in run_module\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/api_server.py\", line 40, in <module>\n",
      "    from vllm.config import VllmConfig\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config/__init__.py\", line 17, in <module>\n",
      "    from vllm.config.model import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config/model.py\", line 24, in <module>\n",
      "    from vllm.transformers_utils.config import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/transformers_utils/config.py\", line 28, in <module>\n",
      "    from transformers.models.auto.image_processing_auto import get_image_processor_config\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/image_processing_auto.py\", line 27, in <module>\n",
      "    from ...image_processing_utils import ImageProcessingMixin\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\", line 22, in <module>\n",
      "    from .image_transforms import center_crop, normalize, rescale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\", line 48, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\", line 467, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/__init__.py\", line 2, in <module>\n",
      "    from keras.api import DTypePolicy\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/__init__.py\", line 34, in <module>\n",
      "    from keras.api import visualization\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n",
      "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
      "    from matplotlib import patches  # For legend patches\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "\n",
      "Waiting for vLLM startup... (9/360)\n",
      "Error on attempt 10: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a2404dcdd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/__init__.py\", line 2, in <module>\n",
      "    from keras.api import DTypePolicy\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/__init__.py\", line 34, in <module>\n",
      "    from keras.api import visualization\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n",
      "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
      "    from matplotlib import patches  # For legend patches\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:15 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:16 [gpu_model_runner.py:3255] Starting to load model /kaggle/tmp/Qwen2.5-32B-Instruct-AWQ...\n",
      "\u001b[1;36m(Worker_TP1 pid=227)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "\n",
      "Waiting for vLLM startup... (10/360)\n",
      "Error on attempt 11: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a2404edf50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/__init__.py\", line 2, in <module>\n",
      "    from keras.api import DTypePolicy\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/__init__.py\", line 34, in <module>\n",
      "    from keras.api import visualization\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n",
      "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
      "    from matplotlib import patches  # For legend patches\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:15 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:16 [gpu_model_runner.py:3255] Starting to load model /kaggle/tmp/Qwen2.5-32B-Instruct-AWQ...\n",
      "\u001b[1;36m(Worker_TP1 pid=227)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:09<00:39,  9.89s/it]\n",
      "\n",
      "Waiting for vLLM startup... (11/360)\n",
      "Error on attempt 12: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a2404efc90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "    from keras.api import DTypePolicy\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/__init__.py\", line 34, in <module>\n",
      "    from keras.api import visualization\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n",
      "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
      "    from matplotlib import patches  # For legend patches\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:15 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:16 [gpu_model_runner.py:3255] Starting to load model /kaggle/tmp/Qwen2.5-32B-Instruct-AWQ...\n",
      "\u001b[1;36m(Worker_TP1 pid=227)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:09<00:39,  9.89s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:23<00:35, 11.91s/it]\n",
      "\n",
      "Waiting for vLLM startup... (12/360)\n",
      "Error on attempt 13: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a25b727390>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "    from keras.api import visualization\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n",
      "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
      "    from matplotlib import patches  # For legend patches\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:15 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:16 [gpu_model_runner.py:3255] Starting to load model /kaggle/tmp/Qwen2.5-32B-Instruct-AWQ...\n",
      "\u001b[1;36m(Worker_TP1 pid=227)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:09<00:39,  9.89s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:23<00:35, 11.91s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:33<00:22, 11.03s/it]\n",
      "\n",
      "Waiting for vLLM startup... (13/360)\n",
      "Error on attempt 14: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a2404dad10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "    from keras.api import visualization\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n",
      "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
      "    from matplotlib import patches  # For legend patches\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:15 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:16 [gpu_model_runner.py:3255] Starting to load model /kaggle/tmp/Qwen2.5-32B-Instruct-AWQ...\n",
      "\u001b[1;36m(Worker_TP1 pid=227)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:09<00:39,  9.89s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:23<00:35, 11.91s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:33<00:22, 11.03s/it]\n",
      "\n",
      "Waiting for vLLM startup... (14/360)\n",
      "Error on attempt 15: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a2404dc0d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "    from keras.src.visualization.plot_bounding_box_gallery import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n",
      "    from matplotlib import patches  # For legend patches\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:15 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:16 [gpu_model_runner.py:3255] Starting to load model /kaggle/tmp/Qwen2.5-32B-Instruct-AWQ...\n",
      "\u001b[1;36m(Worker_TP1 pid=227)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:09<00:39,  9.89s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:23<00:35, 11.91s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:33<00:22, 11.03s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:48<00:12, 12.61s/it]\n",
      "\n",
      "Waiting for vLLM startup... (15/360)\n",
      "Error on attempt 16: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a2404ec990>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:15 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:16 [gpu_model_runner.py:3255] Starting to load model /kaggle/tmp/Qwen2.5-32B-Instruct-AWQ...\n",
      "\u001b[1;36m(Worker_TP1 pid=227)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:09<00:39,  9.89s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:23<00:35, 11.91s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:33<00:22, 11.03s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:48<00:12, 12.61s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:58<00:00, 11.78s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:58<00:00, 11.71s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:05:16 [default_loader.py:314] Loading weights took 58.66 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:05:17 [gpu_model_runner.py:3334] Model loading took 9.0925 GiB memory and 59.077874 seconds\n",
      "\n",
      "Waiting for vLLM startup... (16/360)\n",
      "Error on attempt 17: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a2404e27d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:15 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:16 [gpu_model_runner.py:3255] Starting to load model /kaggle/tmp/Qwen2.5-32B-Instruct-AWQ...\n",
      "\u001b[1;36m(Worker_TP1 pid=227)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:09<00:39,  9.89s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:23<00:35, 11.91s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:33<00:22, 11.03s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:48<00:12, 12.61s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:58<00:00, 11.78s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:58<00:00, 11.71s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:05:16 [default_loader.py:314] Loading weights took 58.66 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:05:17 [gpu_model_runner.py:3334] Model loading took 9.0925 GiB memory and 59.077874 seconds\n",
      "\n",
      "Waiting for vLLM startup... (17/360)\n",
      "Error on attempt 18: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a25b7eab10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "üìÑ Last 20 lines of log:\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n",
      "AttributeError: _ARRAY_API not found\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ERROR 11-28 08:04:14 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "INFO 11-28 08:04:15 [parallel_state.py:1208] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:50451 backend=nccl\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:15 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "WARNING 11-28 08:04:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 7.5 not supported, communicator is not available.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-28 08:04:16 [parallel_state.py:1394] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:16 [gpu_model_runner.py:3255] Starting to load model /kaggle/tmp/Qwen2.5-32B-Instruct-AWQ...\n",
      "\u001b[1;36m(Worker_TP1 pid=227)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:04:17 [cuda.py:377] Using AttentionBackendEnum.TRITON_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:09<00:39,  9.89s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:23<00:35, 11.91s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:33<00:22, 11.03s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:48<00:12, 12.61s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:58<00:00, 11.78s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:58<00:00, 11.71s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m \n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:05:16 [default_loader.py:314] Loading weights took 58.66 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:05:17 [gpu_model_runner.py:3334] Model loading took 9.0925 GiB memory and 59.077874 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=226)\u001b[0;0m INFO 11-28 08:05:43 [gpu_worker.py:359] Available KV cache memory: 2.71 GiB\n",
      "\n",
      "Waiting for vLLM startup... (18/360)\n",
      "‚úÖ vLLM is up on attempt 19!\n"
     ]
    }
   ],
   "source": [
    "# Log file names\n",
    "vllm_logfile = 'vllm.log'\n",
    "\n",
    "# 1) Launch vLLM server in background\n",
    "# vllm_cmd = [\n",
    "#     \"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "#     \"--model\", LOCAL_DIR,\n",
    "#     \"--served-model-name\", \"Qwen3-32B\",\n",
    "#     \"--dtype\", \"float16\",             # ‚Üê use float16 on L4\n",
    "#     \"--gpu-memory-utilization\", \"0.9\",\n",
    "#     \"--tensor-parallel-size\", \"4\",\n",
    "#     \"--enforce-eager\",\n",
    "#     \"--max-model-len\", \"16384\",\n",
    "#     # \"--max-num-seqs\", \"1024\",\n",
    "#     #\"--enable-reasoning\",\n",
    "#     # \"--reasoning-parser\", \"deepseek_r1\",\n",
    "#     \"--enable-auto-tool-choice\", \n",
    "#     \"--tool-call-parser\", \"hermes\",\n",
    "#     \"--host\", \"0.0.0.0\",\n",
    "#     \"--port\", \"8000\",\n",
    "# ]\n",
    "\n",
    "vllm_cmd = [\n",
    "    \"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "    \"--model\", LOCAL_DIR,#\"/kaggle/input/qwen-3/transformers/32b/1\",\n",
    "    \"--served-model-name\", \"Qwen2.5-32B-Instruct-AWQ\",\n",
    "    \"--dtype\", \"float16\",             # ‚Üê use float16 on T4\n",
    "    \"--gpu-memory-utilization\", \"0.9\",\n",
    "    \"--tensor-parallel-size\", \"2\",\n",
    "    \"--enforce-eager\",\n",
    "    \"--max-model-len\", \"16384\",\n",
    "    # \"--reasoning-parser\", \"deepseek_r1\",\n",
    "    # \"--enable-auto-tool-choice\", \n",
    "    # \"--tool-call-parser\", \"hermes\",\n",
    "    \"--host\", \"0.0.0.0\",\n",
    "    \"--port\", \"8000\",\n",
    "]\n",
    "\n",
    "with open(vllm_logfile, \"w\") as logf:\n",
    "    proc = subprocess.Popen(vllm_cmd, stdout=logf, stderr=subprocess.STDOUT)\n",
    "\n",
    "# 2) Poll /v1/models up to 360 times, waiting 10s between tries (max 1 hour)\n",
    "for attempt in range(360):\n",
    "    try:\n",
    "        resp = requests.get(\"http://127.0.0.1:8000/v1/models\", timeout=10)\n",
    "        if resp.status_code == 200:\n",
    "            print(f\"‚úÖ vLLM is up on attempt {attempt+1}!\")\n",
    "            break\n",
    "    except requests.RequestException as e:\n",
    "       print(f\"Error on attempt {attempt+1}: {e}\")\n",
    "       with open(vllm_logfile, \"r\") as f:\n",
    "           lines = f.readlines()\n",
    "       print(\"üìÑ Last 20 lines of log:\")\n",
    "       print(\"\".join(lines[-50:]))\n",
    "    print(f\"Waiting for vLLM startup... ({attempt+1}/360)\")\n",
    "    time.sleep(10)\n",
    "else:\n",
    "    print(\"‚ùå Still not up after 1 hour. Check\", vllm_logfile)\n",
    "    proc.terminate()\n",
    "    raise RuntimeError(\"vLLM startup timeout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:05:53.429345Z",
     "iopub.status.busy": "2025-11-28T08:05:53.429104Z",
     "iopub.status.idle": "2025-11-28T08:05:55.207820Z",
     "shell.execute_reply": "2025-11-28T08:05:55.205883Z",
     "shell.execute_reply.started": "2025-11-28T08:05:53.429322Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Public URL for your vLLM API: https://suboral-gertha-autoicous.ngrok-free.dev                    \n"
     ]
    }
   ],
   "source": [
    "secrets = UserSecretsClient()\n",
    "ngrok.set_auth_token(secrets.get_secret(\"ngrok_key\"))\n",
    "\n",
    "# 3) Open an HTTP tunnel on port 8000\n",
    "tunnel = ngrok.connect(8000, \"http\")\n",
    "public_url = tunnel.public_url\n",
    "# 4) Print the public URL\n",
    "print(\"üöÄ Public URL for your vLLM API:\", public_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:05:55.209053Z",
     "iopub.status.busy": "2025-11-28T08:05:55.208785Z",
     "iopub.status.idle": "2025-11-28T08:05:55.213429Z",
     "shell.execute_reply": "2025-11-28T08:05:55.211938Z",
     "shell.execute_reply.started": "2025-11-28T08:05:55.209031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    " # !curl http://localhost:8000/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:05:55.214394Z",
     "iopub.status.busy": "2025-11-28T08:05:55.214050Z",
     "iopub.status.idle": "2025-11-28T08:05:55.961724Z",
     "shell.execute_reply": "2025-11-28T08:05:55.960730Z",
     "shell.execute_reply.started": "2025-11-28T08:05:55.214358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Public URL for your vLLM API: https://suboral-gertha-autoicous.ngrok-free.dev/v1\n",
      "Qwen2.5-32B-Instruct-AWQ\n"
     ]
    }
   ],
   "source": [
    "openAI_key = secrets.get_secret(\"OpenAI_key\")\n",
    "api_base = public_url.rstrip(\"/\") + \"/v1\" \n",
    "client = OpenAI(base_url=api_base, api_key=openAI_key)\n",
    "models = requests.get(f\"{api_base}/models\").json()[\"data\"]\n",
    "model_id = models[0][\"id\"]\n",
    "print(\"üöÄ Public URL for your vLLM API:\", api_base)\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"üîÑ B·∫ÆT ƒê·∫¶U KEEP-ALIVE LOOP\")\n",
    "print(\"‚ö†Ô∏è  ƒê·ª™NG ƒê√ìNG TAB N√ÄY! ƒê·ªÉ gi·ªØ session Kaggle s·ªëng.\\n\")\n",
    "\n",
    "while True:\n",
    "    print(\"‚öôÔ∏è Still running...\")\n",
    "    time.sleep(60)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11376393,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "datasetId": 8706485,
     "sourceId": 13689268,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 803.741372,
   "end_time": "2024-10-27T06:12:38.713054",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-27T05:59:14.971682",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3ae3375ceefd42f9aeede66494e08172": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_712906b89a034aa981c78ec917695c16",
        "IPY_MODEL_6e02449d72574cb59a7408135ddf0606",
        "IPY_MODEL_aabe4ff8b2734c9c86a33deb7a3d0a52"
       ],
       "layout": "IPY_MODEL_d865f8a6c163451da7719a6d6e720f96"
      }
     },
     "63273838b2cf407a9d8a4b3d3c0a6096": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "64efbe72d2c24a98b72db29e311e7206": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6e02449d72574cb59a7408135ddf0606": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_805d7686fa99450fa5f28d53a2e50938",
       "max": 11,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c76df798b0e0495abc6131f26e897eca",
       "value": 11
      }
     },
     "712906b89a034aa981c78ec917695c16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_af9c9877f8c347788ca34cdd8170fc94",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_c9c855af5c4941d6bed6542ce711a012",
       "value": ""
      }
     },
     "805d7686fa99450fa5f28d53a2e50938": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aabe4ff8b2734c9c86a33deb7a3d0a52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_63273838b2cf407a9d8a4b3d3c0a6096",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_64efbe72d2c24a98b72db29e311e7206",
       "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá11/11‚Äá[04:33&lt;00:00,‚Äá26.87s/it]\n"
      }
     },
     "af9c9877f8c347788ca34cdd8170fc94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c76df798b0e0495abc6131f26e897eca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c9c855af5c4941d6bed6542ce711a012": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d865f8a6c163451da7719a6d6e720f96": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
